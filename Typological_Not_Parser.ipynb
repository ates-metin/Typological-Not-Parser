{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This part installs Berkeley Neural Parser (Benepar) and an English lemmatizer (Lemminflect)."
      ],
      "metadata": {
        "id": "zIOcrJ8J6X4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQOYtcveMZFo"
      },
      "outputs": [],
      "source": [
        "# benepar from: https://github.com/nikitakit/self-attentive-parser\n",
        "!pip install benepar\n",
        "!pip install spacy\n",
        "!python -m spacy download 'en_core_web_md'\n",
        "import en_core_web_md\n",
        "\n",
        "# english lemmatizer from: https://github.com/bjascob/LemmInflect\n",
        "!pip3 install lemminflect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part installs and imports the relevant packages such as getLemma, nltk, regEx, benepar_en3, etc."
      ],
      "metadata": {
        "id": "cmGxjSvs6vx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnpMWJQC-XnL"
      },
      "outputs": [],
      "source": [
        "from lemminflect import getLemma    # lemmatizer for english\n",
        "\n",
        "# requirements for benepar\n",
        "import spacy, benepar      \n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "\n",
        "benepar.download('benepar_en3')\n",
        "\n",
        "if spacy.__version__.startswith('2'):\n",
        "    nlp.add_pipe(BeneparComponent(\"benepar_en3\"))\n",
        "else:\n",
        "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "#     #     #     #     #\n",
        "\n",
        "import re\n",
        "import warnings      # this disables unnecessary warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# matplotlib to draw charts of data\n",
        "\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import matplotlib.rcsetup as rcsetup\n",
        "print(rcsetup.all_backends)\n",
        "\n",
        "# numpy\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function definition of the parser."
      ],
      "metadata": {
        "id": "BF-MSdit67EH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gAjlMc-XL88K"
      },
      "outputs": [],
      "source": [
        "# code taken from Berkeley Neural Parser - Spacy\n",
        "def parse(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  sent = list(doc.sents)[0]\n",
        "  parsed = sent._.parse_string\n",
        "  return parsed\n",
        "\n",
        "def ask_karahan():  # the points we fail\n",
        "  return 'ask karahan'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "an example:"
      ],
      "metadata": {
        "id": "gw_E1mt6pSo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parse('i love hot dogs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1AkWH_BHIEAU",
        "outputId": "267791cf-c605-49ed-c98c-989e87b5a506"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(S (NP (PRP i)) (VP (VBP love) (NP (JJ hot) (NNS dogs))))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to find the relevant constituents of a given English sentence."
      ],
      "metadata": {
        "id": "N2EvQ5YU7CTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "18QUodeuJbLr"
      },
      "outputs": [],
      "source": [
        "# finds the subject\n",
        "def find_subject(sentence):\n",
        "  '''[ENG] Finds the subject for a given string. Finds the NP or sentencial labeled\n",
        "  ('SBAR' 'S') constituents to avoid adverbs and such. \n",
        "  Returns a spacy_token type object.'''\n",
        "\n",
        "  doc = nlp(sentence.strip())\n",
        "  sent = list(doc.sents)[0]\n",
        "  labeled = label_children(sent)\n",
        "  for item in labeled:\n",
        "    if item == ['NP']:\n",
        "      indx = labeled.index(item)\n",
        "      return list(sent._.children)[indx]\n",
        "    elif item == ['SBAR']:\n",
        "      indx = labeled.index(item)\n",
        "      return list(sent._.children)[indx]\n",
        "    elif item == ['S']:\n",
        "      indx = labeled.index(item)\n",
        "      return list(sent._.children)[indx]\n",
        "    elif 'S' in item:\n",
        "      indx = labeled.index(item)\n",
        "      return list(sent._.children)[indx]     \n",
        "    else:\n",
        "      return ask_karahan()\n",
        "\n",
        "# finds the direct object (kind of?)\n",
        "def find_direct_object(sentence):\n",
        "  '''[ENG] Finds direct object for a given string. Finds it according to the position\n",
        "  of the VP. Returns a spacy_token type object. '''\n",
        "\n",
        "  doc = nlp(sentence.strip())\n",
        "  sent = list(doc.sents)[0]\n",
        "  l_vp = find_lowest_vp(sentence)\n",
        "  return list(l_vp._.children)[1]\n",
        "\n",
        "# returns the XP's immediately dominated\n",
        "def label_children(sent) -> list: \n",
        "  '''[ENG] Returns the labels of constituents which are immediately dominated \n",
        "  for a given spacy_token type object, returns a list''' \n",
        "\n",
        "  children = []\n",
        "  for item in list(sent._.children):\n",
        "    label = list(item._.labels)\n",
        "    children.append(label)\n",
        "  return children\n",
        "\n",
        "# finds the lowest vp \n",
        "def find_lowest_vp(sentence):\n",
        "  '''[ENG] Find the lowest VP for a given string. This includes several nested\n",
        "  VP's as well. Returns a spacy_token type object.'''\n",
        "\n",
        "  doc = nlp(sentence.strip())\n",
        "  sent = list(doc.sents)[0]\n",
        "  there_is_VP = True\n",
        "  while there_is_VP:\n",
        "    labeled = label_children(sent)   \n",
        "    if ['VP'] in labeled:  # detects VP among constituents\n",
        "      indx = labeled.index(['VP'])\n",
        "      sent = list(sent._.children)[indx]\n",
        "    else:   # there is no VP \n",
        "      there_is_VP = False\n",
        "      \n",
        "  return sent\n",
        "\n",
        "# finds the v head of the lowest vp\n",
        "def find_verb(sentence):\n",
        "  '''[ENG] Finds the V-head of the lowest VP (the leftmost constituent)\n",
        "  for a given string. Returns a spacy_token type object.'''\n",
        "\n",
        "  return list(find_lowest_vp(sentence))[0]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word order function. "
      ],
      "metadata": {
        "id": "gvpQqA0-7Htg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HJptx-dVitce"
      },
      "outputs": [],
      "source": [
        "def get_word_order(gloss, translation):\n",
        "  '''Returns word order for a given gloss, translation respectively'''\n",
        "\n",
        "  vhead = str(find_verb(translation))\n",
        "  vhead = getLemma(vhead, upos='VERB')[0]\n",
        "  \n",
        "  subj = str(find_subject(translation))\n",
        "  subj.strip()\n",
        "  a_list = []\n",
        "  if ' ' in subj:       # if the subject consists of several words, \n",
        "    subj = re.split(r'\\s', subj)     #takes the longest one\n",
        "    for word in subj:\n",
        "      a_list.append(len(word))\n",
        "    mx = max(a_list)\n",
        "    indx = a_list.index(mx)\n",
        "    subj = getLemma(subj[indx],upos='VERB')[0]\n",
        "  else:\n",
        "    subj = getLemma(subj, upos='VERB')[0]\n",
        "\n",
        "  do = str(find_direct_object(translation))\n",
        "  do = getLemma(do, upos='VERB')[0]\n",
        "  do.strip()\n",
        "  another_list = []\n",
        "  if ' ' in do:            # if the direct object consists of several words,\n",
        "    do = re.split(r'\\s', do)  # takes the longest one\n",
        "    for word in do:\n",
        "      another_list.append(len(word))\n",
        "    max1 = max(another_list)\n",
        "    indx1 = another_list.index(max1)\n",
        "    do = getLemma(do[indx1], upos='NOUN')[0]\n",
        "  else:\n",
        "    do = getLemma(do, upos='NOUN')[0]\n",
        "\n",
        "  if (vhead and do) in gloss:\n",
        "    v = gloss.index(vhead)\n",
        "    o = gloss.index(do)\n",
        "\n",
        "    if subj in gloss:\n",
        "      s = gloss.index(subj)\n",
        "      \n",
        "      SVO = s < v < o\n",
        "      SOV = s < o < v\n",
        "      VOS = v < o < s\n",
        "      VSO = v < s < o \n",
        "      OVS = o < v < s\n",
        "      OSV = o < s < v \n",
        "      \n",
        "      if SVO:\n",
        "        return 'SVO'\n",
        "      elif SOV:\n",
        "        return 'SOV'\n",
        "      elif VOS:\n",
        "        return 'VOS'\n",
        "      elif VSO:\n",
        "        return 'VSO'\n",
        "      elif OVS:\n",
        "        return 'OVS'\n",
        "      elif OSV:\n",
        "        return 'OSV'\n",
        "    \n",
        "    elif 'pro' in gloss:\n",
        "      s = gloss.index('pro')\n",
        "\n",
        "      SVO = s < v < o\n",
        "      SOV = s < o < v\n",
        "      VOS = v < o < s\n",
        "      VSO = v < s < o \n",
        "      OVS = o < v < s\n",
        "      OSV = o < s < v \n",
        "    \n",
        "      if SVO:\n",
        "        return 'SVO'\n",
        "      elif SOV:\n",
        "        return 'SOV'\n",
        "      elif VOS:\n",
        "        return 'VOS'\n",
        "      elif VSO:\n",
        "        return 'VSO'\n",
        "      elif OVS:\n",
        "        return 'OVS'\n",
        "      elif OSV:\n",
        "        return 'OSV'\n",
        "    \n",
        "    else:\n",
        "      return ask_karahan()\n",
        "  \n",
        "  else:\n",
        "    return ask_karahan()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts accusative and genitive into nominative for identifying words in gloss."
      ],
      "metadata": {
        "id": "nn1LVPF-Mfo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nominative(string):\n",
        "  '''Converts pronouns with case to nominative form.'''\n",
        "  \n",
        "  s=re.sub(\"^me$\",\"i\",string)\n",
        "  s=re.sub(\"^my$\",\"i\",s)\n",
        "  s=re.sub(\"^your$\",\"you\",s)\n",
        "  s=re.sub(\"^him$\",\"he\",s)\n",
        "  s=re.sub(\"^his$\",\"he\",s)\n",
        "  s=re.sub(\"^her$\",\"she\",s)\n",
        "  s=re.sub(\"^them$\",\"they\",s)\n",
        "  s=re.sub(\"^us$\",\"we\",s)\n",
        "  s=re.sub(\"^our$\",\"we\",s)\n",
        "\n",
        "  s=re.sub(\"^me \",\"i \",s)\n",
        "  s=re.sub(\"^my \",\"i \",s)\n",
        "  s=re.sub(\"^your \",\"you \",s)\n",
        "  s=re.sub(\"^him \",\"he \",s)\n",
        "  s=re.sub(\"^his \",\"he \",s)\n",
        "  s=re.sub(\"^her \",\"she \",s)\n",
        "  s=re.sub(\"^them \",\"they \",s)\n",
        "  s=re.sub(\"^us \",\"we \",s)\n",
        "  s=re.sub(\"^our \",\"we \",s)\n",
        "\n",
        "  s=re.sub(\" me$\",\" i\",s)\n",
        "  s=re.sub(\" my$\",\" i\",s)\n",
        "  s=re.sub(\" your$\",\" you\",s)\n",
        "  s=re.sub(\" him$\",\" he\",s)\n",
        "  s=re.sub(\" his$\",\" he\",s)\n",
        "  s=re.sub(\" her$\",\" she\",s)\n",
        "  s=re.sub(\" them$\",\" they\",s)\n",
        "  s=re.sub(\" us$\",\" we\",s)\n",
        "  s=re.sub(\" our$\",\" we\",s)\n",
        "\n",
        "  s=re.sub(\" me \",\" i \",s)\n",
        "  s=re.sub(\" my \",\" i \",s)\n",
        "  s=re.sub(\" your \",\" you \",s)\n",
        "  s=re.sub(\" him \",\" he \",s)\n",
        "  s=re.sub(\" his \",\" he \",s)\n",
        "  s=re.sub(\" her \",\" she \",s)\n",
        "  s=re.sub(\" them \",\" they \",s)\n",
        "  s=re.sub(\" us \",\" we \",s)\n",
        "  s=re.sub(\" our \",\" we \",s)\n",
        "\n",
        "  s=re.sub(\"^me-\",\"i-\",s)\n",
        "  s=re.sub(\"^my-\",\"i-\",s)\n",
        "  s=re.sub(\"^your-\",\"you-\",s)\n",
        "  s=re.sub(\"^him-\",\"he-\",s)\n",
        "  s=re.sub(\"^his-\",\"he-\",s)\n",
        "  s=re.sub(\"^her-\",\"she-\",s)\n",
        "  s=re.sub(\"^them-\",\"they-\",s)\n",
        "  s=re.sub(\"^us-\",\"we-\",s)\n",
        "  s=re.sub(\"^our-\",\"we-\",s)\n",
        "\n",
        "  s=re.sub(\"-me$\",\"-i\",s)\n",
        "  s=re.sub(\"-my$\",\"-i\",s)\n",
        "  s=re.sub(\"-your$\",\"-you\",s)\n",
        "  s=re.sub(\"-him$\",\"-he\",s)\n",
        "  s=re.sub(\"-his$\",\"-he\",s)\n",
        "  s=re.sub(\"-her$\",\"-she\",s)\n",
        "  s=re.sub(\"-them$\",\"-they\",s)\n",
        "  s=re.sub(\"-us$\",\"-we\",s)\n",
        "  s=re.sub(\"-our$\",\"-we\",s)\n",
        "\n",
        "  s=re.sub(\"-me-\",\"-i-\",s)\n",
        "  s=re.sub(\"-my-\",\"-i-\",s)\n",
        "  s=re.sub(\"-your-\",\"-you-\",s)\n",
        "  s=re.sub(\"-him-\",\"-he-\",s)\n",
        "  s=re.sub(\"-his-\",\"-he-\",s)\n",
        "  s=re.sub(\"-her-\",\"-she-\",s)\n",
        "  s=re.sub(\"-them-\",\"-they-\",s)\n",
        "  s=re.sub(\"-us-\",\"-we-\",s)\n",
        "  s=re.sub(\"-our-\",\"-we-\",s)\n",
        "\n",
        "  s=re.sub(\" me-\",\" i-\",s)\n",
        "  s=re.sub(\" my-\",\" i-\",s)\n",
        "  s=re.sub(\" your-\",\" you-\",s)\n",
        "  s=re.sub(\" him-\",\" he-\",s)\n",
        "  s=re.sub(\" his-\",\" he-\",s)\n",
        "  s=re.sub(\" her-\",\" she-\",s)\n",
        "  s=re.sub(\" them-\",\" they-\",s)\n",
        "  s=re.sub(\" us-\",\" we-\",s)\n",
        "  s=re.sub(\" our-\",\" we-\",s)\n",
        "\n",
        "  s=re.sub(\"-me \",\"-i \",s)\n",
        "  s=re.sub(\"-my \",\"-i \",s)\n",
        "  s=re.sub(\"-your \",\"-you \",s)\n",
        "  s=re.sub(\"-him \",\"-he \",s)\n",
        "  s=re.sub(\"-his \",\"-he \",s)\n",
        "  s=re.sub(\"-her \",\"-she \",s)\n",
        "  s=re.sub(\"-them \",\"-they \",s)\n",
        "  s=re.sub(\"-us \",\"-we \",s)\n",
        "  s=re.sub(\"-our \",\"-we \",s)\n",
        "\n",
        "  s=re.sub(\" me\\)\",\" i)\",s)\n",
        "  s=re.sub(\" my\\)\",\" i)\",s)\n",
        "  s=re.sub(\" your\\)\",\" you)\",s)\n",
        "  s=re.sub(\" him\\)\",\" he)\",s)\n",
        "  s=re.sub(\" his\\)\",\" he)\",s)\n",
        "  s=re.sub(\" her\\)\",\" she)\",s)\n",
        "  s=re.sub(\" them\\)\",\" they)\",s)\n",
        "  s=re.sub(\" us\\)\",\" we)\",s)\n",
        "  s=re.sub(\" our\\)\",\" we)\",s)\n",
        "  return s\n",
        "\n",
        "# bunu düzgün bi regex olarak yazmak lazım -ateş\n",
        "# glossda pronounların labelı unique, sadece pronoun olduğunu bildiğimiz şeylere kullanırsak gerek kalmaz -arda\n",
        "# anladım, süper -ateş\n",
        "# amelelik yapıp yazdım -arda"
      ],
      "metadata": {
        "id": "FpWF1tPfIqZR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEN-N and ADJ-N functions with RegEx."
      ],
      "metadata": {
        "id": "7qzL9mHNMbJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_n_order(gloss,translation):\n",
        "  '''Returns GEN-N order for a given gloss, translation respectively.'''\n",
        "  \n",
        "  if \"PRP$\" in parse(translation):\n",
        "    regexstring=re.search(\"PRP\\$ [a-z]+[\\(\\)\\s]+[A-Za-z\\(\\)\\s]+NN [a-z]+\",parse(translation))[0]\n",
        "    genandnoun=re.findall(\"[a-z]+\",regexstring)\n",
        "    genitive=getLemma(genandnoun[0],upos='NOUN')[0]\n",
        "    noun=getLemma(genandnoun[-1],upos='NOUN')[0]\n",
        "    genitive=nominative(genitive)\n",
        "    noun=nominative(noun)\n",
        "    if (genitive in gloss) and (noun in gloss):\n",
        "      if gloss.index(genitive)<gloss.index(noun):\n",
        "        return \"GEN-N\"\n",
        "      elif gloss.index(noun)<gloss.index(genitive):\n",
        "        return \"N-GEN\"\n",
        "      else:\n",
        "        return ask_karahan()\n",
        "    else:\n",
        "      return ask_karahan()\n",
        "\n",
        "  elif \"POS\" in parse(translation):\n",
        "    regexstring=re.search(\"[a-z]+\\) \\(POS ['a-z]+[\\(\\)\\s]+[A-Za-z\\(\\)\\s]+NN [a-z]+\",parse(translation))[0]\n",
        "    genandnoun=re.findall(\"[a-z]+\",regexstring)\n",
        "    genitive=getLemma(genandnoun[0],upos='NOUN')[0]\n",
        "    noun=getLemma(genandnoun[-1],upos='NOUN')[0]\n",
        "    if gloss.index(genitive)<gloss.index(noun):\n",
        "      return \"GEN-N\"\n",
        "    elif gloss.index(noun)<gloss.index(genitive):\n",
        "      return \"N-GEN\"\n",
        "    else:\n",
        "      return ask_karahan()\n",
        "  elif \" of)\" in parse(translation):\n",
        "    regexstring=re.search(\"[a-z]+\\)+ [ \\(A-Z]+ of\\)[ \\(A-Z]+ [a-z]+\",parse(translation))[0]\n",
        "    genandnoun=re.findall(\"[a-z]+\",regexstring)\n",
        "    noun=getLemma(genandnoun[0],upos='NOUN')[0]\n",
        "    genitive=getLemma(genandnoun[-1],upos='NOUN')[0]\n",
        "    if gloss.index(genitive)<gloss.index(noun):\n",
        "      return \"GEN-N\"\n",
        "    if gloss.index(noun)<gloss.index(genitive):\n",
        "      return \"N-GEN\"\n",
        "  else:\n",
        "    return ask_karahan()\n",
        "\n"
      ],
      "metadata": {
        "id": "DChQbiKg3Lu-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adj_n_order(gloss,translation):\n",
        "  '''Returns ADJ-N order for a given gloss, translation respectively.'''\n",
        "  \n",
        "  if \"JJ\" in parse(translation):\n",
        "    regexstring=re.search(\"JJ [a-z]+[\\(\\)\\s]+[A-Z]+ [a-z]+\",parse(translation))[0]\n",
        "    adjandnoun=re.findall(\"[a-z]+\",regexstring)\n",
        "    adjective=getLemma(adjandnoun[0],upos='NOUN')[0]\n",
        "    noun=getLemma(adjandnoun[-1],upos='NOUN')[0]\n",
        "    if (adjective in gloss) and (noun in gloss):\n",
        "      if gloss.index(adjective)<gloss.index(noun):\n",
        "        return \"ADJ-N\"\n",
        "      elif gloss.index(noun)<gloss.index(adjective):\n",
        "        return \"N-ADJ\"\n",
        "      else:\n",
        "        return ask_karahan()\n",
        "    else:\n",
        "      return ask_karahan()\n",
        "  else:\n",
        "    return ask_karahan()\n"
      ],
      "metadata": {
        "id": "3MdkFXDrbZ8p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function to read the given language data and return a list."
      ],
      "metadata": {
        "id": "1UJ5oq4YpZJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(txt):\n",
        "  '''reads data // returns the language name, list of glosses \n",
        "  and the list of tranlsations within a list'''\n",
        "  \n",
        "  f = open(txt,'r')\n",
        "  splitted = f.read().lower().split('\\n')\n",
        "  \n",
        "  data = []\n",
        "  for item in splitted:\n",
        "    if item != '' and item != '\\n':\n",
        "      data.append(item)\n",
        "  language = data[0]\n",
        "  data.remove(language)\n",
        "\n",
        "  glosses = []\n",
        "  translations = []\n",
        "  for i in range(len(data)):\n",
        "    if i % 3 == 1:\n",
        "      glosses.append(data[i])\n",
        "    elif i%3 == 2:\n",
        "      translations.append(data[i])\n",
        "  return [language, glosses, translations]\n"
      ],
      "metadata": {
        "id": "MtUxyJCArL47"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions put together."
      ],
      "metadata": {
        "id": "cFEy5jsTMRi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Typological_Not_Parser(File):\n",
        "  '''gets input as .txt file. See the readme file for glossing and file conventions'''\n",
        "  \n",
        "  data = get_data(File)\n",
        "  glosses = data[1]\n",
        "  translations = data[2]\n",
        "\n",
        "  language = data[0]\n",
        "  print(language.upper())\n",
        "  print()\n",
        "  print('{} datapoints for {}'.format(len(glosses),language))\n",
        "  print()\n",
        "  print('Type a command to continue. The commands are features/test/chart/input/commands/exit.')\n",
        "  print()\n",
        "  print('\\'features\\' shows the head-dependent structures for the given glossed data.')\n",
        "  print('\\'test\\' is the evaluation function showing the accuracy of the program for the given test-data.')\n",
        "  print('\\'chart\\' provides a pie chart for the given feature.')\n",
        "  print('\\'input\\' takes manual input for testing.')\n",
        "  print()\n",
        "  print('Type \\'commands\\' to see commands again.')\n",
        "  print()\n",
        "  print('Type \\'exit\\' to exit. Type \\'commands\\' to see commands again.')\n",
        "  \n",
        "\n",
        "\n",
        "  sentences = []\n",
        "  for i in range(len(glosses)):\n",
        "    sentences.append([glosses[i],translations[i]])\n",
        "\n",
        "  wo_labels = ['SVO','SOV','VSO','VOS','OVS','OSV']\n",
        "  adj_n_labels = ['ADJ-N','N-ADJ']\n",
        "  gen_n_labels = ['GEN-N','N-GEN']\n",
        "  \n",
        "  word_orders = []\n",
        "  adj_n = []\n",
        "  gen_n = []\n",
        "  for sentence in sentences:\n",
        "    \n",
        "    res1 = get_word_order(sentence[0],sentence[1])\n",
        "    if res1 in wo_labels:\n",
        "      word_orders.append(res1)\n",
        "    \n",
        "    res2 = adj_n_order(sentence[0],sentence[1])\n",
        "    if res2 in adj_n_labels:\n",
        "      adj_n.append(res2)\n",
        "\n",
        "    res3 = gen_n_order(sentence[0],sentence[1])\n",
        "    if res3 in gen_n_labels:\n",
        "      gen_n.append(res3)\n",
        "\n",
        "  wo_counts = []\n",
        "  adjn_counts = []\n",
        "  genn_counts = []\n",
        "\n",
        "  ### final items are tuples with frequency, label ###\n",
        "\n",
        "  for i in range(len(wo_labels)):\n",
        "    wo_counts.append(word_orders.count(wo_labels[i]))\n",
        "  \n",
        "  mxcount1 = max(wo_counts)\n",
        "  wo_max = wo_labels[wo_counts.index(mxcount1)]\n",
        "  if len(word_orders) != 0:\n",
        "    wo_final = (mxcount1*100/len(word_orders), wo_max)\n",
        "  else:\n",
        "    wo_final = (0, 'None')\n",
        "  for i in range(len(adj_n_labels)):\n",
        "    adjn_counts.append(adj_n.count(adj_n_labels[i]))\n",
        "  \n",
        "  mxcount2 = max(adjn_counts)\n",
        "  adjn_max = adj_n_labels[adjn_counts.index(mxcount2)]\n",
        "  if len(adj_n) != 0:\n",
        "    adjn_final = (mxcount2*100/len(adj_n), adjn_max)\n",
        "  else:\n",
        "    adjn_final = (0, 'None')\n",
        "  for i in range(len(gen_n_labels)):\n",
        "      genn_counts.append(gen_n.count(gen_n_labels[i]))\n",
        "  \n",
        "  mxcount3 = max(genn_counts)\n",
        "  genn_max = gen_n_labels[genn_counts.index(mxcount3)]\n",
        "  if len(gen_n) != 0:\n",
        "    genn_final = (mxcount3*100/len(gen_n), genn_max)\n",
        "  else:\n",
        "    genn_final = (0, 'None')\n",
        "\n",
        "  ### final items are tuples with frequency, label ###\n",
        "\n",
        "  while True:\n",
        "    print()\n",
        "    print('Type a command to continue.')\n",
        "    print()\n",
        "    command = input().strip()\n",
        "    \n",
        "    if command == 'features':\n",
        "      print()\n",
        "      print('The data exhibit {}% {} order with {} datapoints'.format(wo_final[0],wo_final[1],len(word_orders)))\n",
        "      print('The data exhibit {}% {} order with {} datapoints.'.format(adjn_final[0],adjn_final[1],len(adj_n)))\n",
        "      print('The data exhibit {}% {} order with {} datapoints.'.format(genn_final[0],genn_final[1],len(gen_n)))\n",
        "\n",
        "    elif command == 'chart':\n",
        "      print()\n",
        "      feature = input('Please enter a feature (WO, GEN-N, ADJ-N): ')\n",
        "      print()\n",
        "      if feature == 'WO':\n",
        "        y = np.array([wo_counts[0],wo_counts[1],wo_counts[2],wo_counts[3],\n",
        "            wo_counts[4],wo_counts[5]])\n",
        "        plt.pie(y, labels = wo_labels)\n",
        "        plt.show()\n",
        "\n",
        "      elif feature == 'ADJ-N':\n",
        "        y = np.array([adjn_counts[0],adjn_counts[1]])\n",
        "        plt.pie(y, labels = adj_n_labels)\n",
        "        plt.show()\n",
        "\n",
        "      elif feature == 'GEN-N':\n",
        "        y = np.array([genn_counts[0],genn_counts[1]])\n",
        "      \n",
        "        plt.pie(y, labels = gen_n_labels)\n",
        "        plt.show()\n",
        "      print('Type \\'exit\\' to show chart. The program itself \\\n",
        "will decide if it will show the chart or not.')\n",
        "\n",
        "    elif command == 'test':\n",
        "      print()\n",
        "      test_file = input('Enter the test-file name: ')\n",
        "      print()\n",
        "      print('You may wait a long time for testing, as the program is kind of lazy.')\n",
        "      print()\n",
        "      r = open(test_file,'r')\n",
        "      test_list = r.read().split('\\n')\n",
        "      \n",
        "      test_name = test_list.pop(0)\n",
        "\n",
        "      cleaned = []\n",
        "\n",
        "      for item in test_list:\n",
        "        if item in wo_labels:\n",
        "          cleaned.append(item)\n",
        "        elif item in adj_n_labels:\n",
        "          cleaned.append(item)\n",
        "        elif item in gen_n_labels:\n",
        "          cleaned.append(item)\n",
        "        elif item == 'None' or item == 'NONE':\n",
        "          cleaned.append(item)\n",
        "\n",
        "        wo_test_labels = []\n",
        "        adjn_test_labels = []\n",
        "        genn_test_labels = []\n",
        "\n",
        "        for i in range(len(cleaned)):\n",
        "          if i%3 == 0:\n",
        "            wo_test_labels.append(cleaned[i])\n",
        "          elif i%3 == 1:\n",
        "            adjn_test_labels.append(cleaned[i])\n",
        "          elif i%3 == 2:\n",
        "            genn_test_labels.append(cleaned[i])\n",
        "\n",
        "      ev_counts = []\n",
        "      \n",
        "      for sentence in sentences:\n",
        "        wo1 = get_word_order(sentence[0],sentence[1])\n",
        "        adjn1 = adj_n_order(sentence[0],sentence[1])\n",
        "        genn1 = gen_n_order(sentence[0],sentence[1])\n",
        "\n",
        "        i = sentences.index(sentence)\n",
        "        \n",
        "        if wo1 == wo_test_labels[i]:\n",
        "          ev_counts.append(1)\n",
        "        elif wo1 == 'ask karahan' and wo_test_labels[i] == 'None':\n",
        "          ev_counts.append(1)\n",
        "        else:\n",
        "          ev_counts.append(0)\n",
        "        \n",
        "        if adjn1 == adjn_test_labels[i]:\n",
        "          ev_counts.append(1)\n",
        "        elif adjn1 == 'ask karahan' and adjn_test_labels[i] == 'None':\n",
        "          ev_counts.append(1)\n",
        "        else:\n",
        "          ev_counts.append(0)\n",
        "        \n",
        "        if genn1 == genn_test_labels[i]:\n",
        "          ev_counts.append(1)\n",
        "        elif genn1 == 'ask karahan' and genn_test_labels[i] == 'None':\n",
        "          ev_counts.append(1)\n",
        "        else:\n",
        "          ev_counts.append(0)\n",
        "        \n",
        "      score = np.sum(ev_counts)*100/ len(ev_counts)\n",
        "\n",
        "      print(test_name.upper(),':')\n",
        "      print()\n",
        "      print('The accuracy for the given data is {}%'.format(score))\n",
        "\n",
        "    elif command == 'commands':\n",
        "      print()\n",
        "      print('Commands: features/chart/test/input/exit')\n",
        "    \n",
        "    elif command == 'input':\n",
        "      print()\n",
        "      print('Be careful! Only takes sentences with transitive verbs!')\n",
        "      print()\n",
        "      gl = input('Please enter gloss: ')\n",
        "      print()\n",
        "      tr = input('Please enter translation: ')\n",
        "\n",
        "      wo = get_word_order(gl,tr)\n",
        "      GENN = gen_n_order(gl,tr)\n",
        "      ADJN = adj_n_order(gl,tr)\n",
        "\n",
        "      print()\n",
        "      print('The data exhibit {}, {}, and {} orders.'.format(wo,GENN,ADJN))\n",
        "    elif command == 'exit':\n",
        "      break\n",
        "    else:\n",
        "      print('Invalid input')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vnkQifsy0XcT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Typological_Not_Parser('turkish.txt')"
      ],
      "metadata": {
        "id": "YBXIKY6QVpTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Many thanks to Ümit Atlamaz, Utku Türk and Karahan Şahin for their support."
      ],
      "metadata": {
        "id": "xQGChuI3w5KF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Typological Not-Parser",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}